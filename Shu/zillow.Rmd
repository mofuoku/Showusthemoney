---
title: "Zillow Project"
author: "Shu Zhang"
date: "8/15/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
library(ggplot2)
library(GGally)
library(dplyr)
library(VIM)
library(mice)
library(Hmisc)
library(kknn)
library(lubridate)
library(ggthemes)
library(data.table)
library(caret)
```



## II. Import Data


```{r train}
train <- read.csv('train_2016_v2.csv',header = TRUE, stringsAsFactors = FALSE)
properties <- read.csv('properties_2016.csv', header = TRUE, stringsAsFactors = FALSE)
sample <- read.csv('sample_submission.csv',header = TRUE, stringsAsFactors = FALSE)
```

## III. Structure of the Data
### A. Training Set

### Properties Set

### Sample Set
```{r}
dim(train) # 2985217 obs., 7 variables
head(sample,5)
colnames(sample) <- tolower(colnames(sample))
sum(complete.cases(sample)) # no missing values
```
### Transction Date
```{r}

date <- train %>% mutate(year_month = make_date(year = year(transactiondate),month=month(transactiondate)))

date_ds <- date %>%
  group_by(year_month) %>%
  summarise(count = n())

ggplot(data=date_ds, aes(x=year_month,y=count)) +
  geom_bar(stat="identity",fill="purple1") +
  theme_hc()+
  labs(x="Time", y="Number of Transactions",
       title = "Transaction vs Month")

```

##  Missingness

### Summarize the missing values in the data.

```{r missingValues}
total <- NULL
percent <- NULL
for (i in 1:length(colnames(properties))) {
  total[i] <- sum(is.na(properties[,i]))
  percent[i] <- round(total[i]/nrow(original),4)
}

missingValues <- data.frame(variables=colnames(properties),total=total, percent=percent)
missingValues <- missingValues %>% filter(percent > 0)
# missingValues
```


The visualization of missingness of the data indicates that there are columns which have a huge number of missing values (almost the entire column). On the other hand, columns "taxdelinquencyflag","propertyzoningdesc","propertycountylandusecode","parcelid","hashottuborspa" and "fireplaceflag" have no missing values.


```{r}
ggplot(missingValues, aes(reorder(x=variables,-percent),y=percent)) +
  geom_bar(stat="identity",fill="orangered1")+
  theme_tufte()+
  labs(title="Number of missing values in each column")+
  theme(axis.text.x=element_text(angle=90, hjust=1))
  
```


There are 29 out of 64 columns having more than 15% missing values.
```{r}
sum(missingValues[,"percent"] > 0.15)


```
Let's analyse this to understand how to handle the missing data.


We'll consider that when more than 15% of the data is missing, we should delete the corresponding variable. This means that we will not try to impute the missing data in these cases. The more missing data a variable has, the less important it will be among all the housing features, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). According to this, there is a set of variables (e.g. 'basementsqft', 'poolsizesum', 'finishedsquarefeet13', etc.) that we should delete. The majority of columns to be deleted have more than 80 percent of missing data. "basementsqft", "storytypeid", "yardbuildingsqft26",etc have almost 100% of missing values, so we are safe to delete them.


Moreover, looking closer at the variables, we could say that 

```{r}
deleteVariables <- missingValues %>% filter(percent > 0.15) %>%
  arrange(desc(percent))

```

```{r}
write.csv(deleteVariables,"deletedVariables.csv")
```

### Delete columns that having more than 15% of missing values.
```{r}
# need_col contains only the undelete variables
needed_col <- colnames(properties)[!(colnames(properties) %in% deleteVariables$variables)]


properties_clean <- properties %>%
  select(one_of(needed_col))
dim(properties_clean) # The dataset after reducing the missing-value columns has 29 variables.
```
### Delete columns that are duplicated with other columns.
'calculatedbathnbr' = 'bathroomcnt' <br />
'fips' = 'regionidcounty'<br />
'calculatedfinishedsquarefeet' = 'finishedsquarefeet12'<br />
'fullbathcnt' = 'bathroomcnt'

```{r}

col_drop <- c("fips","calculatedbathnbr","finishedsquarefeet12","fullbathcnt")
cl = properties_clean[ ,-which(colnames(properties_clean) %in% col_drop)]
properties_clean <- cl

```


### Remove rows having more than 60% of missing values.
If an observation has missing records over 80% out of all house features, we will consider it as a "bad observation" due to lack of information. We will delete those rows.

```{r}
h <- properties_clean[rowSums(is.na(properties_clean)) < ncol(properties_clean)*0.6, ]
dim(properties_clean)[1] - dim(h)[1] # We need to remove 11437 rows.
sum(is.na(properties_clean$bathroomcnt)) # 11462 missing values in bathroomcnt

```
The number of missing values in bathroomcnt column is 11462. We found that it is almost equal to the number of rows having more than 60% of missing values (11437 rows).



```{r}
properties_clean <- h
```



### Data Cleaning
We will clean data column by column.
```{r}
dim(properties_clean) # 2973780 obs, 26 cols
names(properties_clean)
```
### Imputation 
(to be continued..)



bathroomcnt, bedroomcnt,  (MAR) 

Mean value imputation
```{r}
imputed.bathroomcnt <- impute(properties_clean$bathroomcnt, mean)
properties_clean$bathroomcnt <- imputed.bathroomcnt

imputed.bedroomcnt <- impute(properties_clean$bedroomcnt, mean)
properties_clean$bedroomcnt <- imputed.bedroomcnt

# sum(is.na(properties_clean$calculatedfinishedsquarefeet))
```
calculatedfinishedsquarefeet 

It has a normal distribution, so it is not 'MNAR', but it is related the number of bathrooms or total sqft. We can impute missing values KNN.
```{r}
hist(log(original_clean$calculatedfinishedsquarefeet))
```

```{r}
plot(properties_clean$bathroomcnt,log(properties_clean$calculatedfinishedsquarefeet))
```

```{r}
#tds <- kknn(calculatedfinishedsquarefeet~., original_clean,original_clean[,-"calculatedfinishedsquarefeet"],k=10,distance=2)
```


### Join Train and Properties
Next, letâ€™s join the two tables together on the parcelid column such that we only include properties that have a response value (logerror) in train.
```{r}
train_set <- left_join(date, properties_clean, by = c("parcelid"="parcelid"))
dim(train_set) # 90275 obs., 28 variables
```
### Make transactiondate column as month object
### Delete parcelid col
```{r}
month = month(train_set$year_month)
train_set$transactiondate <- month
train_set <- train_set[,-c(4,1)]
setnames(train_set, old=c("transactiondate"), new=c("month"))
```
### Dummy Variable Cols
We will map "true"/"false","Y"/"N" variables into 0/1's.

```{r}
train_set$hashottuborspa <- ifelse(train_set$hashottuborspa == "",0,1)
train_set$fireplaceflag <- ifelse(train_set$fireplaceflag == "",0,1)
train_set$taxdelinquencyflag <- ifelse(train_set$taxdelinquencyflag == "",0,1)
```


```{r}
write.csv(train_set,"train_set.csv",row.names=FALSE)
train_set <- read.csv('train_set.csv',header = TRUE, stringsAsFactors = FALSE)

```




### Devide data into categorical, numerical parts
```{r}
categorical <- names(train_set[,sapply(train_set,class)=="integer"])


categorical <- append(categorical,"rawcensustractandblock")
categorical <- append(categorical, "bathroomcnt")
categorical <- categorical[-3]
categorical <- categorical[-13]

numeric_ <- names(train_set)[-which(names(train_set) %in% categorical)]
numeric_ <- numeric_[-8] # rm assessmentyear
numeric_ <- numeric_[-c(4,5,10)] #rm "propertycountylandusecode","propertyzoningdesc","censustractandblock" 


categorical <- append(categorical,"censustractandblock")



categorical_set <- train_set[,categorical]
numeric_set <- train_set[,numeric_]


# rm na in categorical set

numeric_set <- numeric_set[complete.cases(categorical_set),]

categorical_set <- categorical_set[complete.cases(categorical_set),]

# Impute missing values in numeric_set
pre <- preProcess(numeric_set,method = "medianImpute")
numeric_set <- predict(pre,numeric_set) 

numeric_set <- scale(numeric_set)

train_set <- cbind(numeric_set,categorical_set)
#View(train_set)
summary(train_set)
```

```{r}

write.csv(train_set, 'train_set_clean.csv',row.names = FALSE)


```


