---
title: "Zillow Project"
author: "Shu Zhang"
date: "8/15/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
library(ggplot2)
library(GGally)
library(dplyr)
library(VIM)
library(mice)
library(Hmisc)
library(kknn)
library(lubridate)
library(ggthemes)
```



## II. Import Data


```{r train}
train <- read.csv('train_2016_v2.csv',header = TRUE, stringsAsFactors = FALSE)
properties <- read.csv('properties_2016.csv', header = TRUE, stringsAsFactors = FALSE)
sample <- read.csv('sample_submission.csv',header = TRUE, stringsAsFactors = FALSE)
```

## III. Structure of the Data
### A. Training Set

### Properties Set

### Sample Set
```{r}
dim(sample) # 2985217 obs., 7 variables
head(sample,5)
colnames(sample) <- tolower(colnames(sample))
sum(complete.cases(sample)) # no missing values
```
### Transction Date
```{r}

date <- train %>% mutate(year_month = make_date(year = year(transactiondate),month=month(transactiondate)))

date_ds <- date %>%
  group_by(year_month) %>%
  summarise(count = n())

ggplot(data=date_ds, aes(x=year_month,y=count)) +
  geom_bar(stat="identity",fill="tomato1") +
  theme_gdocs()+
  labs(x="Time", y="Number of Transactions",
       title = "Transaction vs Month")

```

##  Missingness

```{r}
# aggr(original)

```


## Summarize the missing values in the data.

```{r missingValues}
total <- NULL
percent <- NULL
for (i in 1:length(colnames(properties))) {
  total[i] <- sum(is.na(properties[,i]))
  percent[i] <- round(total[i]/nrow(original),4)
}

missingValues <- data.frame(variables=colnames(properties),total=total, percent=percent)
missingValues
```


The visualization of missingness of the data indicates that there are columns which have a huge number of missing values (almost the entire column). On the other hand, columns "taxdelinquencyflag","propertyzoningdesc","propertycountylandusecode","parcelid","hashottuborspa" and "fireplaceflag" have no missing values.


```{r}
ggplot(missingValues, aes(reorder(x=variables,-percent),y=percent)) +
  geom_bar(stat="identity",fill="tomato1")+
  coord_flip()+
  theme_tufte()
  
```


There are 29 out of 64 columns having more than 15% missing values.
```{r}
sum(missingValues[,"percent"] > 0.15)


```
Let's analyse this to understand how to handle the missing data.


We'll consider that when more than 15% of the data is missing, we should delete the corresponding variable. This means that we will not try to impute the missing data in these cases. The more missing data a variable has, the less important it will be among all the housing features, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). According to this, there is a set of variables (e.g. 'basementsqft', 'poolsizesum', 'finishedsquarefeet13', etc.) that we should delete. The majority of columns to be deleted have more than 80 percent of missing data. "basementsqft", "storytypeid", "yardbuildingsqft26",etc have almost 100% of missing values, so we are safe to delete them.


Moreover, looking closer at the variables, we could say that 

```{r}
deleteVariables <- missingValues %>% filter(percent > 0.15) %>%
  arrange(desc(percent))

```

```{r}
write.csv(deleteVariables,"deletedVariables.csv")
```

### Delete columns that having more than 15% of missing values.
```{r}
# need_col contains only the undelete variables
needed_col <- colnames(properties)[!(colnames(properties) %in% deleteVariables$variables)]


properties_clean <- properties %>%
  select(one_of(needed_col))
dim(properties_clean) # The dataset after reducing the missing-value columns has 29 variables.
```
### Remove rows having more than 60% of missing values.
If an observation has missing records over 80% out of all house features, we will consider it as a "bad observation" due to lack of information. We will delete those rows.

```{r}
h <- properties_clean[rowSums(is.na(properties_clean)) < ncol(properties_clean)*0.6, ]
dim(properties_clean)[1] - dim(h)[1] # We need to remove 11437 rows.
sum(is.na(properties_clean$bathroomcnt)) # 11462 missing values in bathroomcnt

```
The number of missing values in bathroomcnt column is 11462. We found that it is almost equal to the number of rows having more than 60% of missing values (11437 rows).



```{r}
properties_clean <- h
```



### Data Cleaning
We will clean data column by column.
```{r}
dim(properties_clean) # 2973780 obs, 29 cols
names(properties_clean)
```
### Imputation 
(to be continued..)



bathroomcnt, bedroomcnt,  (MAR) 

Mean value imputation
```{r}
imputed.bathroomcnt <- impute(properties_clean$bathroomcnt, mean)
properties_clean$bathroomcnt <- imputed.bathroomcnt

imputed.bedroomcnt <- impute(properties_clean$bedroomcnt, mean)
properties_clean$bedroomcnt <- imputed.bedroomcnt

sum(is.na(original_clean$calculatedfinishedsquarefeet))
```
calculatedfinishedsquarefeet 

It has a normal distribution, so it is not 'MNAR', but it is related the number of bathrooms or total sqft. We can impute missing values KNN.
```{r}
hist(log(original_clean$calculatedfinishedsquarefeet))
```

```{r}
plot(original_clean$bathroomcnt,log(original_clean$calculatedfinishedsquarefeet))
```

```{r}
#tds <- kknn(calculatedfinishedsquarefeet~., original_clean,original_clean[,-"calculatedfinishedsquarefeet"],k=10,distance=2)
```




Concern: calculatedbathnbr = bathroomcnt ??
We can't simply do mean value imputation, 117475 of missing values. Can we use bathroomcnt only? Do they represent the same thing??
```{r}
write.csv(original_clean,'zillow_clean.csv')
```

